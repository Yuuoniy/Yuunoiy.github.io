---
title: 深度学习笔记
date: 2018-11-12 11:04:04
tags:
---
1
<!-- more -->

### L2 正则化
正则化：通过降低复杂模型的复杂度来防止过拟合
结构风险最小化：以最小化损失和复杂度为目标
L2 正则化公式来量化复杂度，该公式将正则化项定义为所有特征权重的平方和

### Lambda 
增加 lambda 值将增强正则化效果
1. 如果您的 lambda 值过高，则模型会非常简单，但是您将面临数据欠拟合的风险。您的模型将无法从训练数据中获得足够的信息来做出有用的预测。
2. 如果您的 lambda 值过低，则模型会比较复杂，并且您将面临数据过拟合的风险。您的模型将因获得过多训练数据特点方面的信息而无法泛化到新数据。

### 逻辑回归

### 神经网络
####失败案例
##### 梯度消失
较低层（更接近输入）的梯度可能会变得非常小。在深度网络中，计算这些梯度时，可能涉及许多小项的乘积。
当较低层的梯度逐渐消失到 0 时，这些层的训练速度会非常缓慢，甚至不再训练。
ReLU 激活函数有助于防止梯度消失。

##### 梯度爆炸
如果网络中的权重过大，则较低层的梯度会涉及许多大项的乘积。在这种情况下，梯度就会爆炸：梯度过大导致难以收敛。
批标准化可以降低学习速率，因而有助于防止梯度爆炸。
ReLU 单元消失
一旦 ReLU 单元的加权和低于 0，ReLU 单元就可能会停滞。它会输出对网络输出没有任何贡献的 0 激活，而梯度在反向传播算法期间将无法再从中流过。由于梯度的来源被切断，ReLU 的输入可能无法作出足够的改变来使加权和恢复到 0 以上。
降低学习速率有助于防止 ReLU 单元消失。

##### 丢弃正则化
这是称为丢弃的另一种形式的正则化，可用于神经网络。其工作原理是，在梯度下降法的每一步中随机丢弃一些网络单元。丢弃得越多，正则化效果就越强：
0.0 = 无丢弃正则化。
1.0 = 丢弃所有内容。模型学不到任何规律。
0.0 和 1.0 之间的值更有用。
### 优化器
AdaGrad 优化器是一种备选方案。AdaGrad 的核心是灵活地修改模型中每个系数的学习率，从而单调降低有效的学习率。该优化器对于凸优化问题非常有效，但不一定适合非凸优化问题的神经网络训练。您可以通过指定 AdagradOptimizer（而不是 GradientDescentOptimizer）来使用 AdaGrad。请注意，对于 AdaGrad，您可能需要使用较大的学习率。

对于非凸优化问题，Adam 有时比 AdaGrad 更有效。要使用 Adam，请调用 tf.train.AdamOptimizer 

### Softmax 层
Softmax 层是紧挨着输出层之前的神经网络层。

### pandas 
### numpy
是一种用于进行科学计算的包 

### 参考资料
1. https://www.slideshare.net/tw_dsconf/ss-62245351?qid=108adce3-2c3d-4758-a830-95d0a57e46bc&v=&b=&from_search=3
2. 
